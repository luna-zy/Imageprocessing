{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we'll need to use OpenCV with Darknet (the CNN defined for Yolo v3 algorithm). Since OpenCV already has darknet in last version, we'll use a Jupyter notebook to execute our examples.\n",
    "\n",
    "In Yolo v3 home page (https://pjreddie.com/darknet/yolo/) we can find tutorials to configure and run both Yolo and Darknet. We are going to use pre-trained model of darknet that uses COCO dataset (http://cocodataset.org/). Darknet has been trained with 80 different classes\n",
    "\n",
    "Firstly, we need to download the Yolo v3 configuration files \n",
    "\n",
    "1. The Yolo v3 configuration file (yolo.cfg)\n",
    "2. The weights for darknet (yolo.weights)\n",
    "3. The names of trained classes (coco.names)\n",
    "\n",
    "Then we have to use python. I recommend you using Jupyter notebook (as I will do in this guide).  We need to import opencv and numpy. If we don't have opencv installed, we can install from notebook (!pip install opencv-python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then we load yolo v3 in cv2.dnn.readNet. We need to use weights and cfg to create the CNN in memory, and load class names as an array. You have to remember that files should be in the folder where you are executing the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = cv2.dnn.readNet('yolov3.weights','yolov3.cfg')\n",
    "classes  = []\n",
    "with open('coco.names','r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We get the ouputs of the darknet CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "outputlayers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate an array of random colors, one for each class with uniform distribution from numpy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors= numpy.random.uniform(0,255,size=(len(classes),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the test image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"dog.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Yolo v3 works with small images (416x416 in this example) we may need reduce our input image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.resize(img,None,fx=0.4,fy=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain dimensions of input image, and extract the three bands, we will uses 'blob' in the CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "height,width,channels = img.shape\n",
    "blob = cv2.dnn.blobFromImage(img,0.00392,(416,416),(0,0,0),True,crop=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can inspect our image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"input image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect scaled band of the input image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in blob:\n",
    "    for n, img_blob in enumerate(b):\n",
    "        cv2.imshow(str( n), img_blob)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to execute the CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02118016 0.02388134 0.04664548 ... 0.         0.         0.        ]\n",
      " [0.01726332 0.01875127 0.38844633 ... 0.         0.         0.        ]\n",
      " [0.02118462 0.01801873 0.07650209 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.9731635  0.9751837  0.05149293 ... 0.         0.         0.        ]\n",
      " [0.9797624  0.9754399  0.30574453 ... 0.         0.         0.        ]\n",
      " [0.97900224 0.9831845  0.0811379  ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "net.setInput(blob)\n",
    "outs = net.forward(outputlayers)\n",
    "print(outs[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to correctly generate output, we need to interpret output\n",
    "Values coming from CNN are normalized in range [0..1]. We'll receive one line per \n",
    "each possible detected object, with the following data:\n",
    " - bounding box center x\n",
    " - bounding box center y\n",
    " - bounding box width\n",
    " - bounding box height\n",
    " - rest of columns are the confidences for each class id\n",
    "Thus we could use a python code to build bounding boxes of identified objects and \n",
    "put the names of classes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing info on screen/ get confidence score of algorithm in detecting an object in blob\n",
    "import numpy as np\n",
    "\n",
    "class_ids=[]\n",
    "confidences=[]\n",
    "boxes=[]\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            #object detected\n",
    "            center_x= int(detection[0]*width)\n",
    "            center_y= int(detection[1]*height)\n",
    "            w = int(detection[2]*width)\n",
    "            h = int(detection[3]*height)\n",
    "            #rectangle co-ordinaters\n",
    "            x=int(center_x - w/2)\n",
    "            y=int(center_y - h/2)\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "            \n",
    "            boxes.append([x,y,w,h]) #put all rectangle areas\n",
    "            confidences.append(float(confidence)) \n",
    "            #how confidence was that object detected and show that percentage\n",
    "            class_ids.append(class_id) #name of the object tha was detected\n",
    "\n",
    "indexes = cv2.dnn.NMSBoxes(boxes,confidences,0.4,0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we can paint on the image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = classes[class_ids[i]].strip()  # <-- corrected here\n",
    "        confidence = confidences[i]\n",
    "        color = colors[i]\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "        # Prepare text for display\n",
    "        text = f\"{label}: {confidence:.2f}\"\n",
    "\n",
    "        # Compute text size\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_TRIPLEX, 0.5, 1)\n",
    "\n",
    "        # Background rectangle for readability\n",
    "        cv2.rectangle(img, (x, y - text_height - baseline), (x + text_width, y), color, thickness=cv2.FILLED)\n",
    "\n",
    "        # Draw label text\n",
    "        cv2.putText(img, text, (x, y - baseline),\n",
    "                    cv2.FONT_HERSHEY_TRIPLEX,\n",
    "                    0.5,\n",
    "                    (255, 255, 255),\n",
    "                    thickness=1)\n",
    "\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
